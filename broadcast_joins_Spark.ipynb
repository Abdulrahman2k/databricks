{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6NhqG5Fc8TAHRiFTu6wKZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdulrahman2k/databricks/blob/master/broadcast_joins_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_17e97jznUO",
        "colab_type": "text"
      },
      "source": [
        "## **Install Java, Spark, and Findspark**\n",
        "This installs Apache Spark 2.3.1, Java 8, and Findspark, a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QPUwnB7wR_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "e0b2c280-d6d5-4581-dc0f-63349ed05f55"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget http://www-us.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz   \n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz   \n",
        "!pip install -q findspark\n",
        "\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-01 13:00:28--  http://www-us.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
            "Resolving www-us.apache.org (www-us.apache.org)... 40.79.78.1\n",
            "Connecting to www-us.apache.org (www-us.apache.org)|40.79.78.1|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz [following]\n",
            "--2020-09-01 13:00:28--  https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 88.99.95.219, 2a01:4f8:10a:201a::2\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|88.99.95.219|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224453229 (214M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.0.0-bin-hadoop3.2.tgz’\n",
            "\n",
            "spark-3.0.0-bin-had 100%[===================>] 214.05M  27.6MB/s    in 8.5s    \n",
            "\n",
            "2020-09-01 13:00:37 (25.2 MB/s) - ‘spark-3.0.0-bin-hadoop3.2.tgz’ saved [224453229/224453229]\n",
            "\n",
            "sample_data  spark-3.0.0-bin-hadoop3.2\tspark-3.0.0-bin-hadoop3.2.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8IrHfoIzyZA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## **Set Environment Variables**\n",
        "Set the locations where Spark and Java are installed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6xU7uUawXR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4jRCPVvxdYq",
        "colab_type": "text"
      },
      "source": [
        "## **Start a SparkSession**\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqrcwOmixlBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext \n",
        "from pyspark import SparkConf \n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName('pyspark') \n",
        "sc = SparkContext(conf=conf) \n",
        "sqc = SQLContext(sc)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUJFsWzsx7SK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4fd40c0-48a7-410c-a432-784a8fe83ed3"
      },
      "source": [
        "\n",
        "my_dict = {\"item1\": 1, \"item2\": 2, \"item3\": 3, \"item4\": 4} \n",
        "my_list = [\"item1\", \"item2\", \"item3\", \"item4\"]\n",
        "\n",
        "my_dict_bc = sc.broadcast(my_dict)\n",
        "\n",
        "def my_func(letter):\n",
        "    return my_dict_bc.value[letter] \n",
        "\n",
        "my_list_rdd = sc.parallelize(my_list)\n",
        "\n",
        "result = my_list_rdd.map(lambda x: my_func(x)).collect()\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTsONJjjx77J",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}